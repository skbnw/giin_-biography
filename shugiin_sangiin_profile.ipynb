{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 衆院HPから議員データを取得\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "from lxml import html\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# 入力と出力のCSVファイルパス\n",
        "input_csv_path = 'Shugiin.csv'\n",
        "output_csv_path = 'Shugiin_output.csv'\n",
        "\n",
        "# 出力ファイルのヘッダー\n",
        "output_header = ['kanji', 'kana', 'url', '議員名', 'かな', '選挙区', '会派', '略歴']\n",
        "\n",
        "# CSV読み込みとデータ処理\n",
        "with open(input_csv_path, 'r', encoding='cp932') as infile:\n",
        "    reader = csv.DictReader(infile)\n",
        "\n",
        "    # 空のデータフレームを作成\n",
        "    df = pd.DataFrame(columns=output_header)\n",
        "\n",
        "    for row in reader:\n",
        "        url = row['url']\n",
        "\n",
        "        try:\n",
        "            # URLにアクセス\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            tree = html.fromstring(response.content)\n",
        "\n",
        "            # 議員名とかなを取得\n",
        "            name_element = tree.xpath('//h2[@id=\"TopContents\"]')\n",
        "            name_text = name_element[0].text_content().strip() if name_element else \"N/A\"\n",
        "            # 名前とかなの分割\n",
        "            if '（' in name_text and '）' in name_text:\n",
        "                name_parts = name_text.split('（')\n",
        "                name_kanji = name_parts[0].strip()\n",
        "                name_kana = f\"（{name_parts[1].replace('）','').strip()}）\"\n",
        "            else:\n",
        "                name_kanji = \"N/A\"\n",
        "                name_kana = \"N/A\"\n",
        "\n",
        "            # 選挙区と会派を取得\n",
        "            district_party_element = tree.xpath('//h2[@id=\"TopContents\"]/following-sibling::br[1]/following-sibling::text()[1]')\n",
        "            district_party_text = district_party_element[0].strip() if district_party_element else \"N/A\"\n",
        "            if '選出、' in district_party_text:\n",
        "                district_party_parts = district_party_text.split('選出、')\n",
        "                district = district_party_parts[0].strip()\n",
        "                party = district_party_parts[1].strip()\n",
        "            else:\n",
        "                district = \"N/A\"\n",
        "                party = \"N/A\"\n",
        "\n",
        "            # 略歴を取得 - <br><br> 以降を選択\n",
        "            bio_elements = tree.xpath('//h2[@id=\"TopContents\"]/following-sibling::br[3]/following-sibling::text()')\n",
        "            bio_text = ''.join([bio.strip() for bio in bio_elements]).strip() if bio_elements else \"N/A\"\n",
        "\n",
        "            # 結果を辞書に格納\n",
        "            result = {\n",
        "                'kanji': row['kanji'],\n",
        "                'kana': row['kana'],\n",
        "                'url': url,\n",
        "                '議員名': name_kanji,\n",
        "                'かな': name_kana,\n",
        "                '選挙区': district,\n",
        "                '会派': party,\n",
        "                '略歴': bio_text.replace('\\n', '').replace('○', '。')\n",
        "            }\n",
        "\n",
        "            # デバッグ用出力\n",
        "            print(f\"Result for {url}: {result}\")\n",
        "\n",
        "            # データフレームに追加\n",
        "            df = pd.concat([df, pd.DataFrame([result])], ignore_index=True)\n",
        "\n",
        "            # スリープ（1～3秒のランダム）\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing URL {url}: {e}\")\n",
        "\n",
        "# 結果をCSVに保存\n",
        "df.to_csv(output_csv_path, encoding='cp932', index=False)\n",
        "\n",
        "print(\"CSV出力が完了しました。\")\n"
      ],
      "metadata": {
        "id": "cQHmq6KqwN2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 参院ＨＰから略歴を取得\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "from lxml import html\n",
        "import time\n",
        "import random\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "input_csv_path = 'Sangiin.csv'\n",
        "output_csv_path = 'Sangiin_output.csv'\n",
        "\n",
        "# 出力ファイルのヘッダー\n",
        "output_header = ['num', 'kanji', 'kanji2', 'kana', 'kaiha', 'url', 'name', 'title', 'biography']\n",
        "\n",
        "# CSV読み込みとデータ処理\n",
        "with open(input_csv_path, 'r', encoding='cp932') as infile, open(output_csv_path, 'w', encoding='cp932', newline='') as outfile:\n",
        "    reader = csv.DictReader(infile)\n",
        "    writer = csv.DictWriter(outfile, fieldnames=output_header)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in reader:\n",
        "        url = row['url']\n",
        "\n",
        "        try:\n",
        "            # URLにアクセス\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            tree = html.fromstring(response.content)\n",
        "\n",
        "            # 氏名の抽出\n",
        "            name = tree.xpath('//div/div/div/div/div/h1')\n",
        "            name_text = name[0].text_content().strip() if name else \"氏名が見つかりません\"\n",
        "\n",
        "            # title の抽出（//ddから3つのコラムを取得）\n",
        "            titles = tree.xpath('//dd')\n",
        "            titles_text = ' '.join([t.text_content().strip() for t in titles[:3]]) if titles else \"タイトルが見つかりません\"\n",
        "\n",
        "            # 略歴の抽出\n",
        "            biography = tree.xpath('//p')\n",
        "            biographies = [b.text_content().strip() for b in biography] if biography else [\"略歴が見つかりません\"]\n",
        "            biographies_text = ' '.join(biographies)\n",
        "\n",
        "            # 結果の書き込み\n",
        "            writer.writerow({\n",
        "                'num': row['num'],\n",
        "                'kanji': row['kanji'],\n",
        "                'kanji2': row['kanji2'],\n",
        "                'kana': row['kana'],\n",
        "                'kaiha': row['kaiha'],\n",
        "                'url': url,\n",
        "                'name': name_text,\n",
        "                'title': titles_text,\n",
        "                'biography': biographies_text\n",
        "            })\n",
        "\n",
        "            # スリープ（1～3秒のランダム）\n",
        "            time.sleep(random.uniform(1, 3))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing URL {url}: {e}\")\n",
        "\n",
        "print(\"CSV出力が完了しました。\")\n"
      ],
      "metadata": {
        "id": "67k2OQOZXfgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bb894d-9691-46cc-ab6d-657daedf76e2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV出力が完了しました。\n"
          ]
        }
      ]
    }
  ]
}